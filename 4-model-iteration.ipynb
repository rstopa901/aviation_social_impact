{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2680c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression, Lasso, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#import spacy\n",
    "#spacy.prefer_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f954f24-da91-4342-9bd8-899240784f84",
   "metadata": {},
   "source": [
    "## Notebook Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43250704-c44d-4677-acf3-f7d3fec044d7",
   "metadata": {},
   "source": [
    "The objective of this notebook is to fine-tune the model created in 3-modelling. As a general overview:\n",
    "* The data is subsetted to accidents where a second pilot is present. This is used as a proxy metric to look at commercial flights. The Federal Aviation Administration (FAA) requires two pilots at all times for most aircraft that exceed 12,500 pounds (source below).\n",
    "* Data pre-process (including NLP) is repeated, utilizing functions from 3-modelling notebook\n",
    "* y variable is changed - now, we are classifying Fatal or Serious accidents (y=1) versus Minor or No Accidents (y=0)\n",
    "* Model parameters are tuned in gridsearch slightly differently to adjust for new data\n",
    "* Results are interpreted\n",
    "\n",
    "Source: https://flygv.com/blog/the-importance-and-benefits-of-utilizing-dual-pilot-operations#:~:text=The%20Federal%20Aviation%20Administration%20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7424ede4-9467-4f6a-9991-1114e360fea4",
   "metadata": {},
   "source": [
    "(the functions below are the same as in 3-modelling notebook and used for pre-processing and modelling in this notebook):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90d4a220-9fd0-4765-ad88-862d45803d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_lemmas(text):\n",
    "    '''\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split() if not word.lower() in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298cd0de-bcfb-41b0-bd1c-6831fa154ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_words(doc):\n",
    "    '''\n",
    "    This function creates a string of lemmas in a SpaCy doc that are\n",
    "    not stop words or punctuation, in the order that they appeared.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: A SpaCy doc object\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A string that contain non-stop, non-puntuation, lemmatized words\n",
    "    '''\n",
    "    return \" \".join([token.lemma_ for token in doc if (not token.is_stop) and (not token.is_punct)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab70817-8e53-4d49-85f6-da0467523190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_df_to_vectors(X, n_gram_range=(1,1)):\n",
    "    '''\n",
    "    Converts a dataframe consisting of a single column of text into\n",
    "    a dataframe of vectorized text data with stop words and\n",
    "    conjugation removed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: A dataframe consisting of a single column of text data\n",
    "    n_gram_range: A tuple containing the range of word chunks size to\n",
    "                  use when grouping words for vectorization. Defaults\n",
    "                  to (1,1).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A dataframe with columns for each vectorized n-gram of text, with\n",
    "    values containing that text's tfidf value for the document in the\n",
    "    corresponding row of the original dataframe.\n",
    "    '''\n",
    "    \n",
    "    # Convert text entries into strings of relevant words\n",
    "    X_lemmas = X.applymap(get_relevant_lemmas)\n",
    "    \n",
    "    # Vectorize our text dataset and store it as a DataFrame\n",
    "    tfidf = TfidfVectorizer(ngram_range=n_gram_range)\n",
    "    X_nlp_sparse = tfidf.fit_transform(X_lemmas[0])\n",
    "    X_tfidf = pd.DataFrame(X_nlp_sparse.todense(), columns=tfidf.get_feature_names_out())\n",
    "    return X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2f57ebb-7370-411f-9311-02c23b9c7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_vectors(X, should_combine_nouns=False, n_gram_range=(1,1)):\n",
    "    '''\n",
    "    Converts a dataframe consisting of a single column of text into\n",
    "    a dataframe of vectorized text data with stop words, punctuation,\n",
    "    and conjugation removed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: A dataframe consisting of a single column of text data\n",
    "    should_combine_nouns: A boolean indicating whether or not\n",
    "                          adjacent nouns in the text data should\n",
    "                          be vectorized together. Defaults to False.\n",
    "    n_gram_range: A tuple containing the range of word chunks size to\n",
    "                  use when grouping words for vectorization. Defaults\n",
    "                  to (1,1).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A dataframe with columns for each vectorized n-gram of text, with\n",
    "    values containing that text's tfidf value for the document in the\n",
    "    corresponding row of the original dataframe.\n",
    "    '''\n",
    "    \n",
    "    # Loading in SpaCy english model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    # Deciding whether or not to merge noun chunks\n",
    "    if should_combine_nouns:\n",
    "        nlp.add_pipe(\"merge_noun_chunks\")\n",
    "    \n",
    "    # Converting text data to SpaCy doc objects\n",
    "    X_nlp = X.applymap(nlp)\n",
    "    # Convert text features into strings of relevant words\n",
    "    X_lemmas = X_nlp.applymap(get_relevant_words)\n",
    "    \n",
    "    # Vectorize our reddit dataset and store it as a DataFrame\n",
    "    tfidf = TfidfVectorizer(ngram_range=n_gram_range)\n",
    "    X_nlp_sparse = tfidf.fit_transform(X_lemmas[0])\n",
    "    X_tfidf = pd.DataFrame(X_nlp_sparse.todense(), columns=tfidf.get_feature_names_out())\n",
    "    return X_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e952c28",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9d2a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing cleaned dataset\n",
    "accident_df = pd.read_pickle('./data/cm_vehicles_flattened_joined')\n",
    "accident_df.columns = accident_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dac36f-ff45-456c-a4d9-7e1caec5b863",
   "metadata": {},
   "source": [
    "The data is subsetted to accidents where a second pilot is present below. This is used as a proxy metric to look at commercial flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11de7d90-1932-4794-ba8e-3c1c4d1b911a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>secondpilotpresent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>21691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>4651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       secondpilotpresent\n",
       "False               21691\n",
       "True                 4651"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at values for secondpilotpresent\n",
    "accident_df['secondpilotpresent'].value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecd347c9-6f6b-408b-9cfb-c112ed668ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe for secondpilotpresent data\n",
    "spp_true = accident_df.loc[accident_df['secondpilotpresent']==True].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c119a0a2-6442-4a14-bf99-016a4a5dd291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4651, 58)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at shape of new data\n",
    "spp_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe4a46-1bda-47e7-8661-a0515dfae36a",
   "metadata": {},
   "source": [
    "Note that the new data where a second pilot was present has **4651 rows**, which is significantly smaller than our starting dataset which we modelled with in 3-modelling notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "579dc901-dc4d-41fe-95ae-f68887759b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop secondpilot present as we already filtered on it and we do not need the column anymore\n",
    "spp_true.drop(columns='secondpilotpresent', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11616b7-0d3d-46fa-8971-82602826d9ba",
   "metadata": {},
   "source": [
    "The first thing that we must do is identify the columns of data that are of interest to us and remove or replace any missing values (also known as NaNs) from them. Having any NaN values in our dataset will make it impossible for our future model to understand and accept our data for training. As such, it is crucial that they be addressed beforeheand.\n",
    "\n",
    "However, we should not simply remove all NaNs from our dataset. If we remove any row of data that has a NaN in it, we will be losing a significant amount of data that would otherwise be in the other columns of those rows. To prevent an extreme loss of data, we should only focus on cleaning the columns that we plan to use in our training.\n",
    "\n",
    "Since our model is attempting to determine the factors that affect accident severity, we can use the probable cause reports and analysis narrative reports reports for each accident to train our model on the aspects that distinguish each accident. We should not include the factual narrative report however, as it contains too much text and would make our dataset too large to store in a notebook such as this for use in training.\n",
    "\n",
    "In addition to this text data, we can train our model on the latitude, longitude, time, state, and plane manufacturer (make) of each accident. These contextual factors for each accident are commonly believed to play a significant role in accident severity, so they should be included in our model so we can infer whether or not these factors are as important as many believe them to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1eed527-9187-400d-bc6f-88a470413083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining columns of interest\n",
    "text_columns_of_interest = ['cm_probablecause','analysisnarrative','factualnarrative']\n",
    "non_text_columns_of_interest = ['cm_latitude', 'cm_longitude', 'cm_eventdate', 'cm_state', 'make']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c0fda-2e43-4b2e-8d7b-dc330603879c",
   "metadata": {},
   "source": [
    "Having selected our columns of interest, with an eye towards minimizing the number of columns we use to avoid overfitting our model later on, we can inspect the NaNs present in each column to determine whether or not to remove or replace them.\n",
    "\n",
    "In the cell below, we can see that the cm_probablecause column has an extremely small number of NaNs (<0.1% of the dataset). As such, we can safely remove these NaNs without meaningfully affecting the distribution of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3eb226a-55ad-4ed4-aca4-04e5961ea09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probable cause column has 9 NaN values.\n",
      "The analysis narrative column has 0 NaN values.\n",
      "The factual narrative column has 0 NaN values.\n"
     ]
    }
   ],
   "source": [
    "# Displaying presence of NaN values in key text columns\n",
    "print(f'The probable cause column has {spp_true.cm_probablecause.isna().sum()} NaN values.')\n",
    "print(f'The analysis narrative column has {spp_true.analysisnarrative.isna().sum()} NaN values.')\n",
    "print(f'The factual narrative column has {spp_true.factualnarrative.isna().sum()} NaN values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e56ffa9d-705c-462b-a1bc-34d31cb0cdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing NaNs from columns with text\n",
    "spp_true = spp_true.dropna(subset=text_columns_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec693a24-be43-46c7-b117-c89fadbc0fbd",
   "metadata": {},
   "source": [
    "Moving on to the contextual feature columns, we can see that the cm_latitide, cm_longitude, and cm_state columns are each missing approximately 1% of the dataset's values. As this is an extremely small proportion of the data, these entries can be safely dropped without fear of misrepresenting our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b4e5d8a-9b3c-4043-83c1-c42ee95bb5e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cm_latitude     68\n",
       "cm_longitude    69\n",
       "cm_eventdate     0\n",
       "cm_state        20\n",
       "make             4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying presence of NaN values in key non-desciptive columns\n",
    "spp_true[non_text_columns_of_interest].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0592050-5e31-466b-a8a0-c71536d17707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing NaNs from columns without descriptive text\n",
    "spp_true = spp_true.dropna(subset=non_text_columns_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f8ab842-de7b-46c0-9502-9c3a7f1a5fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4558, 57)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d73a9a-86a2-4a1c-9ab1-da3812f0ef84",
   "metadata": {},
   "source": [
    "## Converting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d635735-3102-462c-9b14-13d9e12cf8a8",
   "metadata": {},
   "source": [
    "Now that our data has been cleaned of NaN values, we can begin converting it into an AI-friendly format that will allow us to better train our model.\n",
    "\n",
    "First, we can convert the date and time string in the cm_eventdate column from a string of text into the number of seconds that have elapsed since epoch (Jan, 1, 1970). This allows us to represent our datetime data as a simple integer value, which our future model will be better able to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d43ebd-2f1f-43d1-b63a-601693892902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting accident date information from strings to seconds since epoch\n",
    "spp_true.loc[:,'cm_eventdate'] = pd.to_datetime(spp_true.cm_eventdate,\n",
    "                                                  format='%Y-%m-%dT%H:%M:%SZ').astype('int64')//1e9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acc8d62-b4c1-4beb-aa75-c96c2262d5c4",
   "metadata": {},
   "source": [
    "Below, we are removing self-referential words from our two text columns because we are not interested in words that would obviously highly influence the classification outcome. For example, \"fatal\" would probably be more related to fatal/serious accidents than minor/no accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f41db6af-a38e-4524-ab22-6e03bc663f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove common words\n",
    "spp_true['analysisnarrative'] = spp_true['analysisnarrative'].str.lower()\n",
    "removed_words = [' fatally', ' wreckage', ' fatal', ' autopsy', ' surviving', ' survive', ' survived', \\\n",
    "                 ' died', ' death', ' witnesses', ' witness', ' medical', ' medicine', ' medication', ' toxicology', \\\n",
    "                 ' toxicological', ' destroyed', ' crash', ' crashed', 'injury', 'injured', 'injuries']\n",
    "for word in removed_words:\n",
    "#spp_true['analysisnarrative'] \n",
    "    spp_true['analysisnarrative']  = spp_true['analysisnarrative'].map(lambda x: x.replace(word, ' '))\n",
    "    spp_true['cm_probablecause']  = spp_true['cm_probablecause'].map(lambda x: x.replace(word, ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17e64025-379c-4414-9dde-8c62a05a1f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train and test sets as well as feature and target sets\n",
    "spp_true_train, spp_true_test = train_test_split(spp_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "032f1c30-47df-4218-963f-acfa745811fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3418, 57), (1140, 57))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spp_true_train.shape, spp_true_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da111c48-d39c-48b8-8b4e-47544fd81b67",
   "metadata": {},
   "source": [
    "In order to further prepare our data for use in our aviation model, we must separate it based on the kind of processing that it will need.\n",
    "\n",
    "The columns cm_probablecause and cm_analysisnarrative both contain large quantities of string data that correspond to a report on the events of a given accident. In order to use this in a model, natural language processing (NLP) methods will need to be applied to convert these strings of text into vectors of word frequencies. As these NLP steps are fundamentally different from the preprocessing that will be required for the remainder of the data, we ought to separate these columns out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "738fdaca-3af7-4cca-9f01-8a7c9eec72fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting relevant data from dataset\n",
    "\n",
    "# Separating out target data\n",
    "spp_true_severity = spp_true_train[['cm_highestinjury']]\n",
    "spp_true_severity_test = spp_true_test[['cm_highestinjury']]\n",
    "# Separating out text data for vectorization\n",
    "spp_true_text_columns = spp_true_train[text_columns_of_interest]\n",
    "spp_true_text_columns_test = spp_true_test[text_columns_of_interest]\n",
    "# Selecting relevant non-text columns\n",
    "spp_true_trimmed = spp_true_train[non_text_columns_of_interest]\n",
    "spp_true_trimmed_test = spp_true_test[non_text_columns_of_interest]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a939cd1-5bcd-4a7c-9484-7d48d743fed6",
   "metadata": {},
   "source": [
    "#### Converting and Combining Data\n",
    "\n",
    "When performing NLP preprocessing, it is important to collect all the text data that we are interested in into a single place. The process of NLP and vectorization undertaken in this project does not rely on the structure of any given sentence, and instead focuses on the frequency with which a word appears. As such, we can feel free to combine our probable cause and analysis narrative report columns into a single column without affecting the final result of our NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8998c126-2d84-4d17-bc83-608c69ff4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining text columns\n",
    "spp_true_text = pd.DataFrame(spp_true_text_columns.cm_probablecause + \" \" + \\\n",
    "                   spp_true_text_columns.analysisnarrative\n",
    "                   #+ \" \" + accident_df_text_columns.factualnarrative\n",
    "                   )\n",
    "spp_true_text_test = pd.DataFrame(spp_true_text_columns_test.cm_probablecause + \" \" + \\\n",
    "                                     spp_true_text_columns_test.analysisnarrative\n",
    "                                     #+ \" \" + accident_df_text_columns_test.factualnarrative\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a9c982-09c3-4992-adc9-81006f13cdc9",
   "metadata": {},
   "source": [
    "\n",
    "### Natural Language Pre-Processing\n",
    "In this natural language processing phase, we will correct three issues that negatively affect the quality of our report data with regards to machine learning. These issues are stop words, conjugation, and the string format.\n",
    "\n",
    "The vast majority of english sentences contain numerous words that mean little in and of themselves and serve only to maintain the grammar and syntax of an english sentence. Such words include 'is' and 'to'. These words are known as stop words, and our fture model will attempt to train on them if they are not removed from our dataset. Since these words to not actually carry any meaning, our model's attempts to fit to these words will only result in overfitting and increased training time, resulting in a worse model overall.\n",
    "\n",
    "In addition to this, many english that do have meaning come in multiple forms depending on the syntax of the sentence around it. Verb conjugation and plural forms are both examples of this. If multiple versions of the same word are allowed to exist in our dataset, our model will attempt to train on each of them as if they were completely different words. This would lead to model attempting to fit to multiple highly correlated features and dividing up its weights across the various forms of a word that appear in the dataset. In order to combat this, we can attempt to convert every word in our dataset to its syntactically neutral form, known as a lemma, through the process of lemmatization.\n",
    "\n",
    "And lastly, all of our report data is represented in string format. That is to say, as blocks of text. This is a significant issue to us, as our machine learning model is incapable of training on any data that is not purely numeric. In order to use our accident report text data in our machine learning model, we need to convert it into a numeric format. This can be accomplished through the process of vectorization, which creates a column for each unique word in our dataset and stores the frequency with which that word appears in each row of our dataset. In this format, our future model will be perfectly capable of training on our report data and inferring the factors that contribute most to accident severity.\n",
    "\n",
    "(repeat from 3-modelling notebook, functions are called in at the top of the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "918ebcc7-6c78-4400-b869-dfbbd7b23204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing text data and removing stop words\n",
    "spp_true_text_lemmas = spp_true_text.applymap(get_relevant_lemmas)\n",
    "spp_true_text_test_lemmas = spp_true_text_test.applymap(get_relevant_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "869233c1-f291-4071-ad6e-abde82074999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing lemmatized text data and storing it as a DataFrame\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(spp_true_text_lemmas[0])\n",
    "spp_true_tfidf = pd.DataFrame(tfidf.transform(spp_true_text_lemmas[0]).todense(),\n",
    "                                 columns = tfidf.get_feature_names_out())\n",
    "spp_true_tfidf_test = pd.DataFrame(tfidf.transform(spp_true_text_test_lemmas[0]).todense(),\n",
    "                                      columns = tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c1e53d-e22d-407e-ba16-21175dbd2e36",
   "metadata": {},
   "source": [
    "## Numeric and Categorical Pre-Processing\n",
    "\n",
    "And with our report data addressed, we can move on to proccessing our contextual data. All of our contextual data falls into one of two groups: numeric data and categorical data. Both of which have their own issues that must be corrected to maximize future model performance.\n",
    "\n",
    "Numeric data such as latitude, longitude, and event time are all represented purely as numbers. Since data types are already compatible with our machine learning models, they theoretically do not need to be modified in order to be used. However, our future models will make use of a group of techniques known as 'regularization' to reduce overfitting and improve performance, and regularization is sensitive to the scale of the data given to the model. To maximize the accuracy of our final model, it would there fore be benefitial to compress all of our numeric data down to one order of magnitude while preserving the variance within the data that our model needs in order to learn. This can be accomplished through the process of scaling.\n",
    "\n",
    "By contrast, categorical data such as the state the accident took place in and the plane's manufacturer are all represented as strings of text, like with our reports. However, these simple strings of text do not need to be pre-processed with NLP. Instead, we can focus directly on encoding them in a numeric format. One extremely direct way is to simply create a column for each possible string of text within cm_state and make that contains a 1 if a row contained that string and 0 if it did not. This is known as One-Hot Encoding, and it is the simplest approach to numerically encoding nominal categorical data.\n",
    "\n",
    "The following cell used a column transformer to scale numeric columns and encode categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b597e98d-47fc-496c-a030-2efb8f15b35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rstop\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:170: UserWarning: Found unknown categories in columns [1] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# One-Hot encoding non-NLP categorical data\n",
    "# and scaling non-NLP numeric data\n",
    "pre_processor = ColumnTransformer(\n",
    "    [('scaler', StandardScaler(), ['cm_latitude', 'cm_longitude', 'cm_eventdate']),\n",
    "     ('onehot', OneHotEncoder(drop = 'first', handle_unknown = 'ignore'), ['cm_state', 'make'])],\n",
    "    verbose_feature_names_out = False\n",
    ")\n",
    "pre_processor.fit(spp_true_trimmed)\n",
    "# Storing processed non-NLP data for merging with NLP data\n",
    "spp_true_trimmed_transformed = pd.DataFrame(pre_processor.transform(spp_true_trimmed).todense(), \n",
    "                                               columns = pre_processor.get_feature_names_out())\n",
    "spp_true_trimmed_transformed_test = pd.DataFrame(pre_processor.transform(spp_true_trimmed_test).todense(), \n",
    "                                                    columns = pre_processor.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4e8e15-9971-4f66-8818-7beb57e36c0b",
   "metadata": {},
   "source": [
    "#### Combining and Saving the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8647226d-1d08-498c-8a6f-4c71f7df532f",
   "metadata": {},
   "source": [
    "Data is saved but not read as this notebook goes right into modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4db51b9d-9916-4297-b8aa-9a166193de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining datasets and saving\n",
    "spp_true_train = pd.concat([spp_true_trimmed_transformed, \n",
    "                               spp_true_tfidf], axis = 1)\n",
    "spp_true_test = pd.concat([spp_true_trimmed_transformed_test, \n",
    "                              spp_true_tfidf_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "101bd9b2-023b-4016-bdb4-9e7c659a35d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving data for use in modelling\n",
    "spp_true_train.to_pickle('./data/spp_true_train')\n",
    "spp_true_test.to_pickle('./data/spp_true_test')\n",
    "spp_true_severity.to_pickle('./data/spp_true_train_severity')\n",
    "spp_true_severity_test.to_pickle('./data/spp_true_test_severity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee45e4f2-db6e-41d0-8158-244bce28baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Importing cleaned and vectorized datasets\n",
    "# X_train_tfidf = pd.read_pickle('./data/spp_true_train')\n",
    "# X_test_tfidf = pd.read_pickle('./data/spp_true_test')\n",
    "# y_train = pd.read_pickle('./data/spp_true_train_severity')\n",
    "# y_test = pd.read_pickle('./data/spp_true_test_severity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee23df39-277d-421b-916c-df6a7481502d",
   "metadata": {},
   "source": [
    "## Iteration on Modelling\n",
    "\n",
    "We have a working model from 3-modelling notebook, however, we have made some changes to the data. We are looking at flights where a second pilot is present, resulting in a dataframe with 4,558 rows. Here, we will also look at changing the predictor classes before iterating on our previous model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be51039-0c2c-4777-a657-f1378dacba6e",
   "metadata": {},
   "source": [
    "Here we are changing our `y` variable compared to 3-modelling notebook. \n",
    "We are making:\n",
    "* the class `y=1` as `cm_highestinjury` = `Fatal` or `Serious`\n",
    "* the class `y=0` as `cm_highestinjury` = `None` or `Minor`\n",
    "\n",
    "We did this as we want to look at fatal or serious cases as \"bad accidents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3508643-d306-4126-a03c-eac07f4e9db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give categories to ys based on cm_highestinjury\n",
    "y_train = ((y_train=='Fatal')|(y_train=='Serious')).astype('int').cm_highestinjury\n",
    "y_test = ((y_test=='Fatal')|(y_test=='Serious')).astype('int').cm_highestinjury"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54313c20-c61e-403b-906d-e02c53c2bb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cm_highestinjury</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cm_highestinjury\n",
       "0              2397\n",
       "1              1021"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at value counts for y_train\n",
    "y_train.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdbf1be-5faf-4e21-bdd1-ae241d18e992",
   "metadata": {},
   "source": [
    "Since our processed data is both extremely large and extremely sparse, we can convert it to a sparse format in order to improve model training speed by representing our data in a more compact way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd972ea6-332f-471a-9912-fabdb60197cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting feature datasets into sparse matrices for time efficiency\n",
    "X_train_sparse = X_train_tfidf.astype(pd.SparseDtype(\"float64\",0)).sparse.to_coo()\n",
    "X_test_sparse = X_test_tfidf.astype(pd.SparseDtype(\"float64\",0)).sparse.to_coo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385cfd98-8c96-4106-8db0-4e57036c2682",
   "metadata": {},
   "source": [
    "#### Establishing the Baseline\n",
    "\n",
    "When training a classifier model, it is important to establish a baseline accuracy that our model must surpass in order to be considered meaningful. This baseline is defined for classifers as being the accuracy that you would get by simply predicting the most common classification (in this case, 'non-fatal') every single time.\n",
    "\n",
    "The following cell displays the baseline accuracy as ~ 0.677. Note that this is much lower than our previous baseline of ~ 0.819."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b552221b-4ba1-49b6-b4bd-d63ebe5e8995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.677193\n",
       "1    0.322807\n",
       "Name: cm_highestinjury, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the majority class (baseline)\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a64f420-a8d2-4655-bf8c-f0b60d1baab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.701287\n",
       "1    0.298713\n",
       "Name: cm_highestinjury, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616151c4-0e05-44f4-9718-b28af01fb18a",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "When selecting a model to train, its important to keep in mind what we need our model to do. While popular models such as Gradient Boosting, Random Forest, and Neural Networks can all boast high predictive power, they do so at the cost of interpretability. Such models can rarely explain the reasoning behind their decisions accurately, especiallly when given sparse data like our own.\n",
    "\n",
    "However, there is a simpler model whose priorities better alight with our own. Logistic Regression is a simple classifier model that employs the principles of linear regression to apply a single numeric weight to each word in our dataset that it uses to predict the odds of an accident being fatal when that word is present. This model allows us to see, in quantifiable terms, the affect that a word has on our model's decision to classify an accident as fatal or not. And when given binary target data in particular, Logistic Regression can communicate which classification a particular word is associated with through the sign of its numeric weight.\n",
    "\n",
    "For this project, where we aim to determine the factors that affect accident severity by examining a trained model, interpretability is thee most important consideration. As such, we will be using Logistic Regression in our modelling process.\n",
    "\n",
    "The following cell trains several Logistic Regression models and selects the one with the best performance for use in future inferrence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a618e-2b84-4827-9e1e-c26890e395d1",
   "metadata": {},
   "source": [
    "The options for `C` in the gridsearch below are adjusted from 3-modelling notebook to search for the best `C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa83393f-2f35-4e6a-9758-3f7caff8e31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9482153306026916\n",
      "Testing accuracy: 0.8517543859649123\n"
     ]
    }
   ],
   "source": [
    "# Defining logistic regression parameters to sweep over\n",
    "logreg_params = {\n",
    "    'penalty':['l1','l2'],\n",
    "    'C':[0.01,0.1,1,2,3,4,5,6,7,8,10,12,14,100]\n",
    "}\n",
    "# GridSearching logistic regression classifiers\n",
    "logreg_grid = GridSearchCV(LogisticRegression(solver='liblinear'), logreg_params, n_jobs=-1)\n",
    "logreg_grid.fit(X_train_sparse, y_train)\n",
    "# Printing out train and test accuracy scores\n",
    "print(f'Training accuracy: {logreg_grid.score(X_train_sparse, y_train)}')\n",
    "print(f'Testing accuracy: {logreg_grid.score(X_test_sparse, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8b2c27-34de-4eaa-8940-76b6b4fecdd2",
   "metadata": {},
   "source": [
    "Looking at the above testing accuracy of approximately 0.85, we can see that our model has outperformed the baseline accuracy of 0.677 and thus has meaningfully learned to predict severity from our data. \n",
    "\n",
    "From the previous model (from 3-modelling notebook), we see an increase in variance as the training accuracy is higher than the testing accuracy by more. However, given that we removed self-referential words and have much a smaller dataset, this makes sense. Also, the baseline for this iteration is much lower. We select this as the final version of our model as it addressess our problem statement better.\n",
    "\n",
    "Given this, we can now look at the features within the model that have the highest weight in order to make inferrences about factors that most contribute to fatal accidents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b25264cc-34fe-4c1b-8bc0-e74da2b4a468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5, solver='liblinear')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the best estimator\n",
    "logreg_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "389ab3c8-3014-49fe-9400-67aff7b1f590",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_weight_df = pd.DataFrame(logreg_grid.best_estimator_.coef_[0], index = X_train_tfidf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df749db3-5470-48e1-8ae5-d18238115a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accident</th>\n",
       "      <td>5.647813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attendant</th>\n",
       "      <td>5.409655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger</th>\n",
       "      <td>4.624972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impact</th>\n",
       "      <td>4.203972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>likely</th>\n",
       "      <td>3.721031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preimpact</th>\n",
       "      <td>3.450029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radar</th>\n",
       "      <td>3.421872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serious</th>\n",
       "      <td>3.287594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ankle</th>\n",
       "      <td>3.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>turbulence</th>\n",
       "      <td>2.981600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spin</th>\n",
       "      <td>2.962404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seriously</th>\n",
       "      <td>2.946551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fa</th>\n",
       "      <td>2.940651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>two</th>\n",
       "      <td>2.918900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make_SCHEINEMAN-VAN BUREN</th>\n",
       "      <td>2.786134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make_BAKER VICTOR R</th>\n",
       "      <td>2.775371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evidence</th>\n",
       "      <td>2.743175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impacted</th>\n",
       "      <td>2.725797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>2.679881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tree</th>\n",
       "      <td>2.662699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maneuver</th>\n",
       "      <td>2.657827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flight</th>\n",
       "      <td>2.651986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>altitude</th>\n",
       "      <td>2.643808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make_S M &amp; T Aircraft</th>\n",
       "      <td>2.604584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>evacuation</th>\n",
       "      <td>2.595878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mechanic</th>\n",
       "      <td>2.565116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make_Czech Sport</th>\n",
       "      <td>2.479664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cabin</th>\n",
       "      <td>2.477242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maneuvering</th>\n",
       "      <td>2.476066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revealed</th>\n",
       "      <td>2.406510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>descended</th>\n",
       "      <td>2.403001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minute</th>\n",
       "      <td>2.374896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make_MARCONI FRED I JR</th>\n",
       "      <td>2.321276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return</th>\n",
       "      <td>2.287841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make_Wildman</th>\n",
       "      <td>2.252759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>angle</th>\n",
       "      <td>2.216266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>site</th>\n",
       "      <td>2.208302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>found</th>\n",
       "      <td>2.191779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>2.180412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airplane</th>\n",
       "      <td>2.169486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight</th>\n",
       "      <td>2.141964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>several</th>\n",
       "      <td>2.101630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make_MCCURRY CHARLES P</th>\n",
       "      <td>2.091533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toward</th>\n",
       "      <td>2.079686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>turn</th>\n",
       "      <td>2.078678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make_AIRBORNE WINDSPORTS PTY LTD</th>\n",
       "      <td>2.075136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fell</th>\n",
       "      <td>2.064560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>decision</th>\n",
       "      <td>2.052976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>belt</th>\n",
       "      <td>2.021496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>saw</th>\n",
       "      <td>2.012143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>another</th>\n",
       "      <td>2.008877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>consistent</th>\n",
       "      <td>2.008270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strainer</th>\n",
       "      <td>1.995363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ed</th>\n",
       "      <td>1.979836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>end</th>\n",
       "      <td>1.970384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>initiate</th>\n",
       "      <td>1.966151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>investigation</th>\n",
       "      <td>1.932819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make_MINO</th>\n",
       "      <td>1.925741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lines</th>\n",
       "      <td>1.918185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>river</th>\n",
       "      <td>1.871398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         0\n",
       "accident                          5.647813\n",
       "attendant                         5.409655\n",
       "passenger                         4.624972\n",
       "impact                            4.203972\n",
       "likely                            3.721031\n",
       "preimpact                         3.450029\n",
       "radar                             3.421872\n",
       "serious                           3.287594\n",
       "ankle                             3.013900\n",
       "turbulence                        2.981600\n",
       "spin                              2.962404\n",
       "seriously                         2.946551\n",
       "fa                                2.940651\n",
       "two                               2.918900\n",
       "make_SCHEINEMAN-VAN BUREN         2.786134\n",
       "make_BAKER VICTOR R               2.775371\n",
       "evidence                          2.743175\n",
       "impacted                          2.725797\n",
       "one                               2.679881\n",
       "tree                              2.662699\n",
       "maneuver                          2.657827\n",
       "flight                            2.651986\n",
       "altitude                          2.643808\n",
       "make_S M & T Aircraft             2.604584\n",
       "evacuation                        2.595878\n",
       "mechanic                          2.565116\n",
       "make_Czech Sport                  2.479664\n",
       "cabin                             2.477242\n",
       "maneuvering                       2.476066\n",
       "revealed                          2.406510\n",
       "descended                         2.403001\n",
       "minute                            2.374896\n",
       "make_MARCONI FRED I JR            2.321276\n",
       "return                            2.287841\n",
       "make_Wildman                      2.252759\n",
       "angle                             2.216266\n",
       "site                              2.208302\n",
       "found                             2.191779\n",
       "data                              2.180412\n",
       "airplane                          2.169486\n",
       "weight                            2.141964\n",
       "several                           2.101630\n",
       "make_MCCURRY CHARLES P            2.091533\n",
       "toward                            2.079686\n",
       "turn                              2.078678\n",
       "make_AIRBORNE WINDSPORTS PTY LTD  2.075136\n",
       "fell                              2.064560\n",
       "decision                          2.052976\n",
       "belt                              2.021496\n",
       "saw                               2.012143\n",
       "another                           2.008877\n",
       "consistent                        2.008270\n",
       "strainer                          1.995363\n",
       "ed                                1.979836\n",
       "end                               1.970384\n",
       "initiate                          1.966151\n",
       "investigation                     1.932819\n",
       "make_MINO                         1.925741\n",
       "lines                             1.918185\n",
       "river                             1.871398"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_weight_df.sort_values(0, ascending=False).head(60) # radar, initiate, attendant, airframe, ankle, canopy, rule, bar, transmission, turbulence, spin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be09adb0-0fc0-416c-8af9-5e0bf20d9610",
   "metadata": {},
   "source": [
    "## Results Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e5051b-8fee-45ce-a071-f08014163bdb",
   "metadata": {},
   "source": [
    "Below, we are looking at features which have high feature importance in our model. We also want to consider the frequency that the feature occurs in our data (a highly important feature that only occurs once likely is not worth making suggestions to regulators on). The function below prints the frequency of the feature in our data (of 4,558 rows) and prints the fraction where the feature occured for severe/fatal accidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2ec93938-7f74-458d-a56a-501e1271b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_word_occurence(df, word):\n",
    "    occurence = len(df.loc[(df['cm_probablecause'].str.contains(word))|(df['analysisnarrative'].str.contains(word))])\n",
    "    serious_fatal_occurence = len(df.loc[((df['cm_probablecause'].str.contains(word))|(df['analysisnarrative'].str.contains(word)))\\\n",
    "            &(df['cm_highestinjury'].isin(['Serious', 'Fatal']))])\n",
    "    \n",
    "    print(f'There are {occurence} rows where cm_probablecause or analysisnarrative contains {word}.')\n",
    "    print(f'{serious_fatal_occurence}, or {round(100*(serious_fatal_occurence/occurence), 2)} % of those, are serious or fatal.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9d0f68-2cce-4c27-ac9b-c81dce29102f",
   "metadata": {},
   "source": [
    "#### \"Radar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "08baa14b-3d75-4fdc-9867-d280c7040f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 281 rows where cm_probablecause or analysisnarrative contains radar.\n",
      "235, or 83.63 % of those, are serious or fatal.\n"
     ]
    }
   ],
   "source": [
    "print_word_occurence(spp_true, 'radar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef725694-3fc6-4ce8-98c1-dfcbb1c1bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "radar_df = spp_true.loc[((spp_true['cm_probablecause'].str.contains('radar'))|(spp_true['analysisnarrative'].str.contains('radar')))\\\n",
    "            &(spp_true['cm_highestinjury'].isin(['Serious', 'Fatal']))][['cm_probablecause', 'analysisnarrative']] #235 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ce01a38-d3b7-4680-b33d-0d1c2a5401ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39764.18850836774"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# radar data altitude\n",
    "np.exp(10.590722)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b242d1c-9dd7-44ac-9931-8acccd0ecde0",
   "metadata": {},
   "source": [
    "The word \"radar\" was of high importance. A one word increase in the occurence of \"radar\" in the flight narrative meant the flight being serious or fatal was 39764 times as likely. \n",
    "This is because in cases where accidents were serious or fatal, radar data was often investigated. Specifically, it was used to see the flight's altitude and if/when radar connection was lost with the flight.\n",
    "\n",
    "An example is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "817bfc53-b89d-4c48-b875-d6c1f1c604b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the pilot's improper in flight planning/decision making, his flight into known icing conditions, and his failure to maintain adequate airspeed which resulted in the inadvertent stall/spin and impact with terrain. Factors contributing to the accident were the pilot's improper pre-flight planning/preparation, the icing conditions, and the inadvertent stall/spin.\n",
      "\n",
      "\n",
      "the airplane departed las vegas, nevada, approximately 0919, on an ifr flight plan to midland, texas. the pilot climbed to an initial cruising altitude of 13,000 feet. at 1005, the pilot contacted albuquerque artcc (zab) and reported that he was level at 13,000 feet. at 1009, the pilot requested to climb to 15,000 and the zab controller approved the request. at 1013:55, the pilot contacted albuquerque flight watch and reported that he was approximately 23 miles west of flagstaff, arizona at 15,000 feet, and that about 20 miles west of his position, at 13,000 feet, he encountered \"light mixed icing.\"  the pilot requested any pirep's. flight watch reported that a pirep for \"a trace of rime icing at 12,000,\" was reported by an airplane climbing westbound out of albuquerque. the pilot acknowledged and asked for the weather across new mexico. flight watch advised the pilot to stand by while he gathered the reports. at 1015:15, the pilot contacted zab. he reported, \"getting...mixed...right...now,\" and requested to climb to 17,000 feet. at 1015:57 the controller cleared the airplane to 17,000 feet. at 1016:35, the fw specialist repeated the report of trace icing near albuquerque. the pilot did not reply. zab radar indicated the airplane climbed to 15,200 feet then entered a rapid descent. at 1017:08, a broken transmission was received. no further communications were received from the airplane. radar contact was lost with the airplane at 1017:20. an examination of the airplane  showed no anomalies. &#x0d;\n",
      "&#x0d;\n",
      "&#x0d;\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in radar_df.iloc[2]:\n",
    "    print(i)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4f4d8-e0d8-420c-897f-bf29a9bf635a",
   "metadata": {},
   "source": [
    "#### \"Airframe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "359df090-21e6-42f3-8f21-21a8af37dcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 651 rows where cm_probablecause or analysisnarrative contains airframe.\n",
      "344, or 52.84 % of those, are serious or fatal.\n"
     ]
    }
   ],
   "source": [
    "print_word_occurence(spp_true, 'airframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d163918-67ff-4506-b300-069b77bb712d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1078.1134638873286"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(6.982968)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb9ad13-09b8-4fb3-ab3d-d5f636f00ae6",
   "metadata": {},
   "source": [
    "\"Airframe\" refers to the mechanical structure of an aircraft and was of high importance in classifying fatal/serious accidents. A one word increase in the occurence of \"airframe\" in the flight narrative or probable cause meant the flight being serious or fatal was 1078 times as likely.\n",
    "\n",
    "source: https://en.wikipedia.org/wiki/Airframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ac6e60-21b8-4069-bfc6-3806baad414e",
   "metadata": {},
   "source": [
    "#### \"Transmission\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0e7e7e09-3bff-470e-a0a9-2e3bcfa47806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 91 rows where cm_probablecause or analysisnarrative contains transmission.\n",
      "56, or 61.54 % of those, are serious or fatal.\n"
     ]
    }
   ],
   "source": [
    "#transmission\n",
    "print_word_occurence(spp_true, 'transmission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0546e0eb-943d-4aaa-a9ab-502ed3820bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.5012503660406"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(4.448531)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119e868-d7b7-4075-a891-7d15ffe343a7",
   "metadata": {},
   "source": [
    "A one word increase in the occurence of \"transmission\" in the flight narrative or probable cause meant the flight being serious or fatal was 86 times as likely. The word \"transmission\" in terms of aircraft can refer to radio transmission or engine transmission, both of which were found in the data. In particular, many flights which were serious/fatal have information about radio transmission in their flight narratives. This is because the radio transmissions are an important component in investigating aircraft accidents and why they happened. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7dd971d1-ff3b-4853-a283-772bf6ea35ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "transmission_df = spp_true.loc[((spp_true['cm_probablecause'].str.contains('transmission'))|(spp_true['analysisnarrative'].str.contains('transmission')))\\\n",
    "            &(spp_true['cm_highestinjury'].isin(['Serious', 'Fatal']))][['cm_probablecause', 'analysisnarrative']] #235 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1438f79c-07b7-41ad-8a8c-c79a1f049706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pilot's failure to maintain terrain clearance while executing an instrument approach.  A factor was the night instrument meteorological conditions.\n",
      "\n",
      "\n",
      "while executing an ils approach in night instrument meteorological conditions, the approach controller instructed the pilot that radar services were terminated, and to switch to the advisory frequency.  the pilot acknowledged the instruction, and no further transmissions were received from the airplane.  review of radar data revealed that the airplane intercepted the final approach course for the runway 3 localizer, where it began a gradual descent.  about 4 minutes prior to the accident, the airplane was recorded on the localizer course, at a ground speed of 115 knots; however, radar coverage was subsequently lost.  the airplane impacted a wooded area about 1/2 mile west of the runway, approximately abeam the 500-foot markers painted on the runway surface.  the  path was oriented approximately 90 degrees left of the inbound approach heading.  a postcrash fire consumed the main .  the weather reported at the airport, about 7 minutes prior to the accident, included 2-1/2 statute miles of visibility; mist; few clouds at 200 feet agl, and an overcast cloud layer at 1,800 feet agl.  the weather reported at the airport, about 1 minute after the accident, included 3/4 statute miles of visibility; mist; scattered clouds at 200 and 500 feet agl, an overcast cloud layer at 1,800 feet agl.  review of the instrument approach procedure revealed that the minimums for the straight in approach were 3/4 statute miles of visibility, and a decision altitude of 250 feet agl.  the missed approach procedure was, climb to 3,000 feet, direct to a vor located about 6.7 miles northeast of the airport, and hold.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in transmission_df.iloc[9]:\n",
    "    print(i)\n",
    "    print('\\n')\n",
    "# engine transmission, radio transmission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fcaa39-1cd7-4f36-8426-4955d1794af2",
   "metadata": {},
   "source": [
    "#### \"Turbulence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d196d611-7c8e-41ce-b6fd-ecf89577e26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 242 rows where cm_probablecause or analysisnarrative contains turbulence.\n",
      "194, or 80.17 % of those, are serious or fatal.\n"
     ]
    }
   ],
   "source": [
    "print_word_occurence(spp_true, 'turbulence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "353928a2-6a5c-43f8-9a9a-b10585b1e591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.59168053440504"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(4.425944)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3659b15b-2bf4-4c92-a420-6f0c2013e51d",
   "metadata": {},
   "source": [
    "A one word increase in the occurence of \"turbulence\" in the flight narrative or probable cause meant the flight being serious or fatal was 84 times as likely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "edc066e4-bb47-42da-bf39-26c3f0e9781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "turbulence_df = spp_true.loc[((spp_true['cm_probablecause'].str.contains('turbulence'))|(spp_true['analysisnarrative'].str.contains('turbulence')))\\\n",
    "            &(spp_true['cm_highestinjury'].isin(['Serious', 'Fatal']))][['cm_probablecause', 'analysisnarrative']] #235 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "edbb7628-c7e2-49ec-8949-06c5ea8004e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pilot not maintaining aircraft control while encountering windshear during initial climbout, resulting in a stall at a low altitude.\n",
      "\n",
      "\n",
      "while on initial climb after takeoff the airplane encountered windshear and the airplane stalled, subsequently impacting the terrain.  the pilot reported the takeoff and initial climb were normal until approximately 300 feet above ground level when the airplane encountered \"severe turbulence, causing left turn and loss of lift.\"  the pilot stated the engine was producing \"full power\" when the airplane impacted the terrain.  the winds were 020 degrees magnetic at 12 knots, with gusts of 17 knots.&#x0d;\n",
      "&#x0d;\n",
      "&#x0d;\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in turbulence_df.iloc[6]:\n",
    "    print(i)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e47ea-473e-4f20-bae0-0fde8ae8a92d",
   "metadata": {},
   "source": [
    "#### \"Installation\" - important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "d48fb400-34e2-45f9-abc8-600c08239c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 189 rows where cm_probablecause or analysisnarrative contains installation.\n",
      "60, or 31.75 % of those, are serious or fatal.\n"
     ]
    }
   ],
   "source": [
    "# canopy\n",
    "print_word_occurence(spp_true, 'installation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "201eef2e-c846-4a38-a904-b9eb6ef305b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.02751872341782"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(3.970811)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a187b996-18bb-4c31-96a0-af4d4503fb8b",
   "metadata": {},
   "source": [
    "A one word increase in the occurence of \"installation\" in the flight narrative or probable cause meant the flight being serious or fatal was 53 times as likely. There are many accidents which report that the probably cause was incorrect installation of some part of the aircraft. \n",
    "\n",
    "It is imperative to improve the training of maintenance personnel/mechanics who install parts on aircraft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "08c4c80c-02bf-4aa4-9905-78ce2ece8222",
   "metadata": {},
   "outputs": [],
   "source": [
    "installation_df = spp_true.loc[((spp_true['cm_probablecause'].str.contains('installation'))|(spp_true['analysisnarrative'].str.contains('installation')))\\\n",
    "            &(spp_true['cm_highestinjury'].isin(['Serious', 'Fatal']))][['cm_probablecause']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ecde25-110b-463b-a198-cbb29c0536e1",
   "metadata": {},
   "source": [
    "Some examples are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "48324b15-23a7-4522-9025-d18364ab929b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The improper installation of the left tailrotor control cable by company maintenance personnel.\n",
      "The improper installation of the helicoil by the non-certificated mechanic, and the inadequate inspection of the installation by the certificated mechanic.\n",
      "A shorted terminal lug on the landing gear hydraulic pump which resulted in a cabin fire. Contributing to the accident was the lack of clear installation procedures for the hydraulic pump.\n",
      "A loss of engine power due to the in-flight separation of the 1-3-5 cylinder induction tube elbow, which was caused by the improper installation of the induction tube elbow by maintenance personnel.\n"
     ]
    }
   ],
   "source": [
    "for i in installation_df.iloc[8]:\n",
    "    print(i)\n",
    "for i in installation_df.iloc[10]:\n",
    "    print(i)\n",
    "for i in installation_df.iloc[19]:\n",
    "    print(i)\n",
    "for i in installation_df.iloc[22]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c719c9e3-1287-4cba-83da-3e60c140d9a2",
   "metadata": {},
   "source": [
    "#### Other terms: \"ankle\", \"bar\", \"canopy\"\n",
    "\n",
    "These terms had high feature importance but were not very frequent in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "26b5eb43-3007-4a66-8c7e-ed53167e4cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 56 rows where cm_probablecause or analysisnarrative contains ankle.\n",
      "55, or 98.21 % of those, are serious or fatal.\n"
     ]
    }
   ],
   "source": [
    "# ankle\n",
    "print_word_occurence(spp_true, 'ankle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "70ce64e1-1b83-4a00-a7d6-8650d363a3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30 rows where cm_probablecause or analysisnarrative contains  bar .\n",
      "10, or 33.33 % of those, are serious or fatal.\n"
     ]
    }
   ],
   "source": [
    "# bar\n",
    "print_word_occurence(spp_true, ' bar ')\n",
    "# steering bar, control bar, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d6ce8-eddf-44da-bb28-6286b8298a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 31 rows where cm_probablecause or analysisnarrative contains canopy.\n",
      "20, or 64.52 % of those, are serious or fatal.\n"
     ]
    }
   ],
   "source": [
    "# canopy\n",
    "print_word_occurence(spp_true, 'canopy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f939b-dfef-4fe5-8fc4-93ff3b9ae14d",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "* \"Radar\", \"transmission\", and \"airframe\" are common words used when investigating serious/fatal aircraft accidents. It is good that the industry uses these metrics consistently in such cases\n",
    "* \"Installation\" is both high-importance and high-frequency as a feature. Looking through examples, we can see that many of the cases for serious/fatal accidents cited \"improper installment\" of some aircraft part as the probable cause for the accident. This should be the focus for regulators and researchers\n",
    "* No make or model of the aircrafts had both high feature importance and high frequency\n",
    "* No weather related terms had high feature importance (this was our guess initially)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
